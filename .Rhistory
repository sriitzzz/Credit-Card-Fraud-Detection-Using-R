install.packages( "./DMwR_0.4.1.tar.gz", repos=NULL, type="source" )
install.packages( "DMwR_0.4.1.tar.gz", repos=NULL, type="source" )
install.packages(c("abind", "ROCR"))
install.packages( "DMwR_0.4.1.tar.gz", repos=NULL, type="source" )
# CREDIT CARD FRAUD DETECTION BY ABHINAV DEEP (1906452)
# IMPORTS
library(dplyr) # for data manipulation
library(stringr) # for data manipulation
library(caret) # for sampling
library(caTools) # for train/test split
library(ggplot2) # for data visualization
library(corrplot) # for correlations
library(Rtsne) # for tsne plotting
library(DMwR) # for smote implementation
library(ROSE)# for ROSE sampling
library(rpart)# for decision tree model
library(Rborist)# for random forest model
library(xgboost) # for xgboost model
# function to set plot height and width
fig <- function(width, heigth){
options(repr.plot.width = width, repr.plot.height = heigth)
}
# loading the data
df = read.csv('./Dataset/creditcard.csv')
# BASIC DATA EXPLORATION
head(df)
str(df)
summary(df)
# checking missing values
colSums(is.na(df))
# checking class imbalance
table(df$Class)
# class imbalance in percentage
prop.table(table(df$Class))
fig(12, 8)
common_theme <- theme(plot.title = element_text(hjust = 0.5, face = "bold"))
ggplot(data = df, aes(x = factor(Class),
y = prop.table(stat(count)), fill = factor(Class),
label = scales::percent(prop.table(stat(count))))) +
geom_bar(position = "dodge") +
geom_text(stat = 'count',
position = position_dodge(.9),
vjust = -0.5,
size = 3) +
scale_x_discrete(labels = c("no fraud", "fraud"))+
scale_y_continuous(labels = scales::percent)+
labs(x = 'Class', y = 'Percentage') +
ggtitle("Distribution of class labels") +
common_theme
# DATA VISUALIZATION
# Distribution of variable 'Time' by class
fig(14, 8)
df %>%
ggplot(aes(x = Time, fill = factor(Class))) + geom_histogram(bins = 100)+
labs(x = 'Time in seconds since first transaction', y = 'No. of transactions') +
ggtitle('Distribution of time of transaction by class') +
facet_grid(Class ~ ., scales = 'free_y') + common_theme
fig(14, 8)
ggplot(df, aes(x = factor(Class), y = Amount)) + geom_boxplot() +
labs(x = 'Class', y = 'Amount') +
ggtitle("Distribution of transaction amount by class") + common_theme
# Correlation of anonymized variables and 'Amount'
fig(14, 8)
correlations <- cor(df[,-1],method="pearson")
corrplot(correlations, number.cex = .9, method = "circle", type = "full", tl.cex=0.8,tl.col = "black")
# Visualization of transactions using t-SNE¶
fig(16, 10)
# Use 10% of data to compute t-SNE
tsne_subset <- 1:as.integer(0.1*nrow(df))
tsne <- Rtsne(df[tsne_subset,-c(1, 31)], perplexity = 20, theta = 0.5, pca = F, verbose = F, max_iter = 500, check_duplicates = F)
classes <- as.factor(df$Class[tsne_subset])
tsne_mat <- as.data.frame(tsne$Y)
ggplot(tsne_mat, aes(x = V1, y = V2)) + geom_point(aes(color = classes)) + theme_minimal() + common_theme + ggtitle("t-SNE visualisation of transactions") + scale_color_manual(values = c("#E69F00", "#56B4E9"))
# Data Preparation
#Remove 'Time' variable
df <- df[,-1]
#Change 'Class' variable to factor
df$Class <- as.factor(df$Class)
levels(df$Class) <- c("Not_Fraud", "Fraud")
#Scale numeric variables
df[,-30] <- scale(df[,-30])
head(df)
# Split data into train and test sets
set.seed(123)
split <- sample.split(df$Class, SplitRatio = 0.7)
train <-  subset(df, split == TRUE)
test <- subset(df, split == FALSE)
# Choosing sampling technique¶
# class ratio initially
table(train$Class)
# downsampling
set.seed(9560)
down_train <- downSample(x = train[, -ncol(train)],
y = train$Class)
table(down_train$Class)
# upsampling
set.seed(9560)
up_train <- upSample(x = train[, -ncol(train)],
y = train$Class)
table(up_train$Class)
# smote
set.seed(9560)
smote_train <- SMOTE(Class ~ ., data  = train)
table(smote_train$Class)
# rose
set.seed(9560)
rose_train <- ROSE(Class ~ ., data  = train)$data
table(rose_train$Class)
# Decision Trees (CART)
# Decision trees on original (imbalanced) dataset
#CART Model Performance on imbalanced data
set.seed(5627)
orig_fit <- rpart(Class ~ ., data = train)
#Evaluate model performance on test set
pred_orig <- predict(orig_fit, newdata = test, method = "class")
roc.curve(test$Class, pred_orig[,2], plotit = TRUE)
# Decision Tree on various sampling techniques
set.seed(5627)
# Build down-sampled model
down_fit <- rpart(Class ~ ., data = down_train)
set.seed(5627)
# Build up-sampled model
up_fit <- rpart(Class ~ ., data = up_train)
set.seed(5627)
# Build smote model
smote_fit <- rpart(Class ~ ., data = smote_train)
set.seed(5627)
# Build rose model
rose_fit <- rpart(Class ~ ., data = rose_train)
# AUC on down-sampled data
pred_down <- predict(down_fit, newdata = test)
print('Fitting model to downsampled data')
roc.curve(test$Class, pred_down[,2], plotit = FALSE)
# AUC on up-sampled data
pred_up <- predict(up_fit, newdata = test)
print('Fitting model to upsampled data')
roc.curve(test$Class, pred_up[,2], plotit = FALSE)
# AUC on up-sampled data
pred_smote <- predict(smote_fit, newdata = test)
print('Fitting model to smote data')
roc.curve(test$Class, pred_smote[,2], plotit = FALSE)
# AUC on up-sampled data
pred_rose <- predict(rose_fit, newdata = test)
print('Fitting model to rose data')
roc.curve(test$Class, pred_rose[,2], plotit = FALSE)
# Models on upsampled data
# Logistic Regression
glm_fit <- glm(Class ~ ., data = up_train, family = 'binomial')
pred_glm <- predict(glm_fit, newdata = test, type = 'response')
roc.curve(test$Class, pred_glm, plotit = TRUE)
# Random Forest
x = up_train[, -30]
y = up_train[,30]
rf_fit <- Rborist(x, y, ntree = 1000, minNode = 20, maxLeaf = 13)
rf_pred <- predict(rf_fit, test[,-30], ctgCensus = "prob")
prob <- rf_pred$prob
roc.curve(test$Class, prob[,2], plotit = TRUE)
# XGBoost
# Convert class labels from factor to numeric
labels <- up_train$Class
y <- recode(labels, 'Not_Fraud' = 0, "Fraud" = 1)
set.seed(42)
xgb <- xgboost(data = data.matrix(up_train[,-30]),
label = y,
eta = 0.1,
gamma = 0.1,
max_depth = 10,
nrounds = 300,
objective = "binary:logistic",
colsample_bytree = 0.6,
verbose = 0,
nthread = 7,
)
xgb_pred <- predict(xgb, data.matrix(test[,-30]))
roc.curve(test$Class, xgb_pred, plotit = TRUE)
names <- dimnames(data.matrix(up_train[,-30]))[[2]]
# Compute feature importance matrix
importance_matrix <- xgb.importance(names, model = xgb)
# Nice graph
xgb.plot.importance(importance_matrix[1:10,])
# With an auc score of 0.977 the XGBOOST model has performed the best though both the random forest and logistic regression models have shown reasonable performance.
# Conclusion
# In this project we have tried to show different methods of dealing with unbalanced datasets like the fraud credit card transaction dataset where the instances of fraudulent cases is few compared to the instances of normal transactions. We have argued why accuracy is not a appropriate measure of model performance here and used the metric AREA UNDER ROC CURVE to evaluate how different methods of oversampling or undersampling the response variable can lead to better model training. We concluded that the oversampling technique works best on the dataset and achieved significant improvement in model performance over the imabalanced data. The best score of 0.977 was achieved using an XGBOOST model though both random forest and logistic regression models performed well too. It is likely that by further tuning the XGBOOST model parametres we can achieve even better performance. However this project has demonstrated the importance of sampling effectively, modelling and predicting data with an imbalanced dataset.
# END
